{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bte1ngygIzYD"
      },
      "source": [
        "# Assignment CNNs - Part 2: Transfer Learning Strategies with Fast Food Classification\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lq_Hc0aIW4-p"
      },
      "source": [
        "In this notebook, we will:\n",
        "1. Import and prepare the Fast Food Classification dataset\n",
        "2. Implement three different transfer learning strategies:\n",
        "   - Feature extraction (frozen pre-trained model)\n",
        "   - Fine-tuning last few layers\n",
        "   - Full fine-tuning\n",
        "3. Get experience on how sample size affects the strategy's performance\n",
        "4. Get experience of the impact of choosing the right/wrong learning rate\n",
        "5. Get experience with the different transfer learning strategies\n",
        "\n",
        "## 0. Introduction\n",
        "\n",
        "Transfer learning is a powerful technique that allows us to leverage pre-trained models for new tasks. However, different transfer learning strategies are suitable for different scenarios, particularly depending on:\n",
        "- Sample size of the target dataset\n",
        "- Similarity between source and target tasks\n",
        "- Available computational resources\n",
        "\n",
        "In this exercise, we'll explore different transfer learning strategies using the \"Fast Food Classification Dataset - V2\" from Kaggle."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tVcGvEOOf5i-"
      },
      "source": [
        "## 1. Data Import and Cleaning\n",
        "\n",
        "1. Download the [\"Fast Food Classification Dataset - V2\"](https://www.kaggle.com/datasets/utkarshsaxenadn/fast-food-classification-dataset/data) from Kaggle.\n",
        "\n",
        "2. Unzip and ensure that the dataset in Colab is structured as follows:\n",
        "\n",
        "```\n",
        "|\n",
        "|-- Fast Food Classification V2\n",
        "|   |-- TFRecords\n",
        "|   |-- Test\n",
        "|   |-- Train\n",
        "|   `-- Valid\n",
        "|-- sample_data\n",
        "`-- archive.zip\n",
        "```\n",
        "\n",
        "`Fast Food Classification V2` and `archive.zip` should be on the same level as the already existing `sample_data` directory from Colab. You can ignore the `TFRecords` subdirectory in the following tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "n5811abzgIiK"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "\n",
            "  0  820M    0 80466    0     0   106k      0  2:11:13 --:--:--  2:11:13  106k\n",
            "  1  820M    1 8589k    0     0  5135k      0  0:02:43  0:00:01  0:02:42 9083k\n",
            "  2  820M    2 20.6M    0     0  7896k      0  0:01:46  0:00:02  0:01:44 10.5M\n",
            "  4  820M    4 33.1M    0     0  9230k      0  0:01:31  0:00:03  0:01:28 11.2M\n",
            "  5  820M    5 45.4M    0     0  9964k      0  0:01:24  0:00:04  0:01:20 11.5M\n",
            "  7  820M    7 57.7M    0     0  10.1M      0  0:01:20  0:00:05  0:01:15 11.6M\n",
            "  8  820M    8 66.2M    0     0  9792k      0  0:01:25  0:00:06  0:01:19 11.0M\n",
            "  9  820M    9 75.4M    0     0   9.8M      0  0:01:23  0:00:07  0:01:16 10.9M\n",
            " 10  820M   10 86.7M    0     0  10.0M      0  0:01:22  0:00:08  0:01:14 10.7M\n",
            " 11  820M   11 98.0M    0     0  10.1M      0  0:01:20  0:00:09  0:01:11 10.5M\n",
            " 13  820M   13  110M    0     0  10.3M      0  0:01:19  0:00:10  0:01:09 10.5M\n",
            " 14  820M   14  122M    0     0  10.5M      0  0:01:17  0:00:11  0:01:06 11.9M\n",
            " 16  820M   16  134M    0     0  10.6M      0  0:01:17  0:00:12  0:01:05 11.8M\n",
            " 17  820M   17  147M    0     0  10.7M      0  0:01:16  0:00:13  0:01:03 12.0M\n",
            " 19  820M   19  159M    0     0  10.8M      0  0:01:15  0:00:14  0:01:01 12.2M\n",
            " 20  820M   20  171M    0     0  10.9M      0  0:01:14  0:00:15  0:00:59 12.2M\n",
            " 22  820M   22  184M    0     0  11.0M      0  0:01:14  0:00:16  0:00:58 12.2M\n",
            " 23  820M   23  196M    0     0  11.1M      0  0:01:13  0:00:17  0:00:56 12.2M\n",
            " 25  820M   25  207M    0     0  11.1M      0  0:01:13  0:00:18  0:00:55 12.1M\n",
            " 26  820M   26  218M    0     0  11.1M      0  0:01:13  0:00:19  0:00:54 11.8M\n",
            " 28  820M   28  230M    0     0  11.1M      0  0:01:13  0:00:20  0:00:53 11.8M\n",
            " 29  820M   29  243M    0     0  11.2M      0  0:01:13  0:00:21  0:00:52 11.8M\n",
            " 31  820M   31  255M    0     0  11.2M      0  0:01:12  0:00:22  0:00:50 11.8M\n",
            " 32  820M   32  267M    0     0  11.3M      0  0:01:12  0:00:23  0:00:49 12.0M\n",
            " 34  820M   34  280M    0     0  11.3M      0  0:01:12  0:00:24  0:00:48 12.3M\n",
            " 35  820M   35  290M    0     0  11.3M      0  0:01:12  0:00:25  0:00:47 11.9M\n",
            " 36  820M   36  302M    0     0  11.3M      0  0:01:12  0:00:26  0:00:46 11.9M\n",
            " 38  820M   38  315M    0     0  11.3M      0  0:01:12  0:00:27  0:00:45 11.9M\n",
            " 39  820M   39  327M    0     0  11.4M      0  0:01:11  0:00:28  0:00:43 11.9M\n",
            " 41  820M   41  340M    0     0  11.4M      0  0:01:11  0:00:29  0:00:42 11.8M\n",
            " 42  820M   42  350M    0     0  11.4M      0  0:01:11  0:00:30  0:00:41 11.9M\n",
            " 44  820M   44  362M    0     0  11.4M      0  0:01:11  0:00:31  0:00:40 11.9M\n",
            " 45  820M   45  374M    0     0  11.4M      0  0:01:11  0:00:32  0:00:39 11.9M\n",
            " 47  820M   47  387M    0     0  11.5M      0  0:01:11  0:00:33  0:00:38 11.9M\n",
            " 48  820M   48  399M    0     0  11.5M      0  0:01:11  0:00:34  0:00:37 11.8M\n",
            " 50  820M   50  411M    0     0  11.5M      0  0:01:11  0:00:35  0:00:36 12.2M\n",
            " 51  820M   51  423M    0     0  11.5M      0  0:01:11  0:00:36  0:00:35 12.1M\n",
            " 53  820M   53  435M    0     0  11.5M      0  0:01:10  0:00:37  0:00:33 12.1M\n",
            " 54  820M   54  448M    0     0  11.5M      0  0:01:10  0:00:38  0:00:32 12.1M\n",
            " 56  820M   56  460M    0     0  11.5M      0  0:01:10  0:00:39  0:00:31 12.1M\n",
            " 57  820M   57  472M    0     0  11.6M      0  0:01:10  0:00:40  0:00:30 12.1M\n",
            " 58  820M   58  484M    0     0  11.6M      0  0:01:10  0:00:41  0:00:29 12.1M\n",
            " 60  820M   60  496M    0     0  11.6M      0  0:01:10  0:00:42  0:00:28 12.1M\n",
            " 61  820M   61  508M    0     0  11.6M      0  0:01:10  0:00:43  0:00:27 12.1M\n",
            " 63  820M   63  520M    0     0  11.6M      0  0:01:10  0:00:44  0:00:26 12.0M\n",
            " 64  820M   64  531M    0     0  11.6M      0  0:01:10  0:00:45  0:00:25 11.8M\n",
            " 66  820M   66  543M    0     0  11.6M      0  0:01:10  0:00:46  0:00:24 11.9M\n",
            " 67  820M   67  555M    0     0  11.6M      0  0:01:10  0:00:47  0:00:23 11.9M\n",
            " 69  820M   69  567M    0     0  11.6M      0  0:01:10  0:00:48  0:00:22 11.8M\n",
            " 70  820M   70  580M    0     0  11.6M      0  0:01:10  0:00:49  0:00:21 11.9M\n",
            " 72  820M   72  593M    0     0  11.7M      0  0:01:10  0:00:50  0:00:20 12.3M\n",
            " 73  820M   73  605M    0     0  11.7M      0  0:01:10  0:00:51  0:00:19 12.3M\n",
            " 75  820M   75  616M    0     0  11.6M      0  0:01:10  0:00:52  0:00:18 12.0M\n",
            " 76  820M   76  628M    0     0  11.7M      0  0:01:10  0:00:53  0:00:17 12.1M\n",
            " 78  820M   78  641M    0     0  11.7M      0  0:01:10  0:00:54  0:00:16 12.1M\n",
            " 79  820M   79  653M    0     0  11.7M      0  0:01:09  0:00:55  0:00:14 12.0M\n",
            " 81  820M   81  665M    0     0  11.7M      0  0:01:09  0:00:56  0:00:13 11.9M\n",
            " 82  820M   82  677M    0     0  11.7M      0  0:01:09  0:00:57  0:00:12 12.2M\n",
            " 84  820M   84  689M    0     0  11.7M      0  0:01:09  0:00:58  0:00:11 12.2M\n",
            " 85  820M   85  702M    0     0  11.7M      0  0:01:09  0:00:59  0:00:10 12.2M\n",
            " 87  820M   87  714M    0     0  11.7M      0  0:01:09  0:01:00  0:00:09 12.2M\n",
            " 88  820M   88  725M    0     0  11.7M      0  0:01:09  0:01:01  0:00:08 11.9M\n",
            " 89  820M   89  737M    0     0  11.7M      0  0:01:09  0:01:02  0:00:07 11.9M\n",
            " 91  820M   91  749M    0     0  11.7M      0  0:01:09  0:01:03  0:00:06 12.0M\n",
            " 92  820M   92  762M    0     0  11.7M      0  0:01:09  0:01:04  0:00:05 11.9M\n",
            " 94  820M   94  774M    0     0  11.7M      0  0:01:09  0:01:05  0:00:04 11.9M\n",
            " 95  820M   95  786M    0     0  11.7M      0  0:01:09  0:01:06  0:00:03 12.2M\n",
            " 97  820M   97  798M    0     0  11.8M      0  0:01:09  0:01:07  0:00:02 12.2M\n",
            " 98  820M   98  811M    0     0  11.8M      0  0:01:09  0:01:08  0:00:01 12.2M\n",
            "100  820M  100  820M    0     0  11.8M      0  0:01:09  0:01:09 --:--:-- 12.2M\n"
          ]
        }
      ],
      "source": [
        "#!/bin/bash\n",
        "!curl -L -o archive.zip https://www.kaggle.com/api/v1/datasets/download/utkarshsaxenadn/fast-food-classification-dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "SytLYfaw6GyZ"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'unzip' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n"
          ]
        }
      ],
      "source": [
        "!unzip -q archive.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53kVJ_3nW4-v"
      },
      "source": [
        "3. Apply the provided `delete_invalid_images` function to check and remove any corrupted or invalid images in each of the three dataset directories."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "SRkFVeV2W4-w"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checking directory: Fast Food Classification V2/Train\n",
            "Deleting invalid or corrupted image: Fast Food Classification V2/Train\\Hot Dog\\Hot Dog - Train (59).jpg\n",
            "Checking directory: Fast Food Classification V2/Valid\n",
            "Checking directory: Fast Food Classification V2/Test\n",
            "Invalid and unsupported image cleanup completed.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "# Define the paths to the directories\n",
        "directories = [\n",
        "    \"Fast Food Classification V2/Train\", #TODO: Set the correct path\n",
        "    \"Fast Food Classification V2/Valid\", #TODO: Set the correct path\n",
        "    \"Fast Food Classification V2/Test\" #TODO: Set the correct path\n",
        "]\n",
        "\n",
        "# Define supported image file extensions\n",
        "supported_extensions = {'.jpg', '.jpeg', '.png', '.gif', '.bmp'}\n",
        "\n",
        "def delete_invalid_images(directory):\n",
        "    \"\"\"Goes through a directory and deletes any invalid or unsupported images.\"\"\"\n",
        "    print(f\"Checking directory: {directory}\")\n",
        "    check_directory = os.path.exists(directory)\n",
        "    if not check_directory:\n",
        "        print(f\"Directory does not exist: {directory}\")\n",
        "        return\n",
        "    for root, _, files in os.walk(directory):\n",
        "        for file in files:\n",
        "            file_path = os.path.join(root, file)\n",
        "            # Check if the file has a supported extension\n",
        "            if not any(file.lower().endswith(ext) for ext in supported_extensions):\n",
        "                print(f\"Deleting unsupported file: {file_path}\")\n",
        "                os.remove(file_path)\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                with Image.open(file_path) as img:\n",
        "                    img.load()  # Ensure the image content can be read\n",
        "                    if img.format not in [\"JPEG\", \"PNG\", \"GIF\", \"BMP\"]:\n",
        "                        raise IOError(\"Unsupported image format\")\n",
        "            except (IOError, SyntaxError, AttributeError) as e:\n",
        "                print(f\"Deleting invalid or corrupted image: {file_path}\")\n",
        "                os.remove(file_path)\n",
        "\n",
        "# Run the function for each directory\n",
        "for dir_path in directories:\n",
        "    delete_invalid_images(dir_path)\n",
        "\n",
        "print(\"Invalid and unsupported image cleanup completed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBvTBaGh6SoJ"
      },
      "source": [
        "4. To reduce training time, only three classes are considered for the following training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "lRKkSOw16SEc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Removing folder: Fast Food Classification V2\\Train\\Baked Potato\n",
            "Removing folder: Fast Food Classification V2\\Train\\Burger\n",
            "Removing folder: Fast Food Classification V2\\Train\\Fries\n",
            "Removing folder: Fast Food Classification V2\\Train\\Hot Dog\n",
            "Removing folder: Fast Food Classification V2\\Train\\Pizza\n",
            "Removing folder: Fast Food Classification V2\\Train\\Sandwich\n",
            "Removing folder: Fast Food Classification V2\\Train\\Taco\n",
            "Removing folder: Fast Food Classification V2\\Test\\Baked Potato\n",
            "Removing folder: Fast Food Classification V2\\Test\\Burger\n",
            "Removing folder: Fast Food Classification V2\\Test\\Fries\n",
            "Removing folder: Fast Food Classification V2\\Test\\Hot Dog\n",
            "Removing folder: Fast Food Classification V2\\Test\\Pizza\n",
            "Removing folder: Fast Food Classification V2\\Test\\Sandwich\n",
            "Removing folder: Fast Food Classification V2\\Test\\Taco\n",
            "Removing folder: Fast Food Classification V2\\Valid\\Baked Potato\n",
            "Removing folder: Fast Food Classification V2\\Valid\\Burger\n",
            "Removing folder: Fast Food Classification V2\\Valid\\Fries\n",
            "Removing folder: Fast Food Classification V2\\Valid\\Hot Dog\n",
            "Removing folder: Fast Food Classification V2\\Valid\\Pizza\n",
            "Removing folder: Fast Food Classification V2\\Valid\\Sandwich\n",
            "Removing folder: Fast Food Classification V2\\Valid\\Taco\n",
            "Folder cleanup completed.\n"
          ]
        }
      ],
      "source": [
        "# prompt: In each of the Train, Test, and Valid folder in the folder \"Fast Food Classification V2\" that were just unzipped, only keep the folders Taquito, Crispy Chicken, and Donut. Remove the others.\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# Define the root directory of the dataset\n",
        "dataset_root = \"Fast Food Classification V2\"\n",
        "\n",
        "# Define the subdirectories (Train, Test, Valid)\n",
        "subdirectories = [\"Train\", \"Test\", \"Valid\"]\n",
        "\n",
        "# Define the classes to keep\n",
        "classes_to_keep = [\"Taquito\", \"Crispy Chicken\", \"Donut\"]\n",
        "\n",
        "# Loop through each subdirectory\n",
        "for subdir in subdirectories:\n",
        "  subdir_path = os.path.join(dataset_root, subdir)\n",
        "\n",
        "  # Loop through each folder (class) in the subdirectory\n",
        "  for class_folder in os.listdir(subdir_path):\n",
        "    class_folder_path = os.path.join(subdir_path, class_folder)\n",
        "\n",
        "    # Check if it's a directory and if it's not one of the classes to keep\n",
        "    if os.path.isdir(class_folder_path) and class_folder not in classes_to_keep:\n",
        "      print(f\"Removing folder: {class_folder_path}\")\n",
        "      shutil.rmtree(class_folder_path)\n",
        "\n",
        "print(\"Folder cleanup completed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmyVO2AmW4-x"
      },
      "source": [
        "### Creation of Different Training Dataset Sizes\n",
        "\n",
        "   - Small: 50 images per class\n",
        "   - Medium: 200 images per class  \n",
        "   - Full: All available training images\n",
        "\n",
        "To reduce training time, only three classes are considered in the training.\n",
        "\n",
        "The validation and test sets will remain constant to ensure fair comparison."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "LUXtbLyXW4-x"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "# Constants\n",
        "SMALL_SAMPLES = 50  # images per class\n",
        "MEDIUM_SAMPLES = 200 # images per class\n",
        "\n",
        "def create_sample_dataset(source_dir, target_dir, samples_per_class):\n",
        "    \"\"\"Creates a smaller dataset by randomly sampling from source directory\"\"\"\n",
        "    if os.path.exists(target_dir):\n",
        "        shutil.rmtree(target_dir)\n",
        "\n",
        "    # Create target directory\n",
        "    os.makedirs(target_dir)\n",
        "\n",
        "    # For each class directory\n",
        "    for class_dir in os.listdir(source_dir):\n",
        "        source_class_path = os.path.join(source_dir, class_dir)\n",
        "        target_class_path = os.path.join(target_dir, class_dir)\n",
        "\n",
        "        if os.path.isdir(source_class_path):\n",
        "            # Create class directory in target\n",
        "            os.makedirs(target_class_path)\n",
        "\n",
        "            # Get list of all images\n",
        "            images = os.listdir(source_class_path)\n",
        "\n",
        "            # Randomly sample specified number of images\n",
        "            selected_images = np.random.choice(\n",
        "                images,\n",
        "                size=min(samples_per_class, len(images)),\n",
        "                replace=False\n",
        "            )\n",
        "\n",
        "            # Copy selected images\n",
        "            for img in selected_images:\n",
        "                shutil.copy2(\n",
        "                    os.path.join(source_class_path, img),\n",
        "                    os.path.join(target_class_path, img)\n",
        "                )\n",
        "\n",
        "# Create sampled datasets\n",
        "create_sample_dataset(\"./Fast Food Classification V2/Train\", \"./Train_Small\", SMALL_SAMPLES)\n",
        "create_sample_dataset(\"./Fast Food Classification V2/Train\", \"./Train_Medium\", MEDIUM_SAMPLES)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0xznoUZW4-y"
      },
      "source": [
        "## 2. Data Preprocessing\n",
        "\n",
        "1. Load the training, validation and test datasets using the `image_dataset_from_directory` function. Specifically, set appropriate values for `label_mode`, `batch_size`, `image_size` and `shuffle`. Each dataset should get a fixed `seed` of `42`.\n",
        "\n",
        "Checkout the documentation for details:\n",
        "https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image_dataset_from_directory\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "h6dKFV2IW4-z"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 150 files belonging to 3 classes.\n",
            "Found 600 files belonging to 3 classes.\n",
            "Found 4500 files belonging to 3 classes.\n",
            "Found 1100 files belonging to 3 classes.\n",
            "Found 400 files belonging to 3 classes.\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Constants\n",
        "LABEL_MODE = 'int'\n",
        "SEED = 42\n",
        "IMAGE_HEIGHT = 128\n",
        "IMAGE_WIDTH = 128\n",
        "#Tip: Models that will be used with this data work usually best when the input images are of the same size as in the original training of the model.\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Load the small dataset\n",
        "train_data_small = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    \"Train_Small\",\n",
        "    label_mode=LABEL_MODE,\n",
        "    seed=SEED,  # Any fixed value works for reproducibility\n",
        "    image_size=(IMAGE_HEIGHT, IMAGE_WIDTH),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle= None #TODO: Set appropriate value\n",
        ")\n",
        "\n",
        "# Load the medium dataset\n",
        "train_data_medium = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    \"Train_Medium\",\n",
        "    label_mode=LABEL_MODE,\n",
        "    seed=SEED,  # Any fixed value works for reproducibility\n",
        "    image_size=(IMAGE_HEIGHT, IMAGE_WIDTH),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=None #TODO: Set appropriate value\n",
        ")\n",
        "\n",
        "# Load the full dataset\n",
        "train_data_full = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    \"Fast Food Classification V2/Train\",\n",
        "    label_mode=LABEL_MODE,\n",
        "    seed=SEED,  # Any fixed value works for reproducibility\n",
        "    image_size=(IMAGE_HEIGHT, IMAGE_WIDTH),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=None #TODO: Set appropriate value\n",
        ")\n",
        "\n",
        "# Load validation dataset\n",
        "validation_data = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    \"Fast Food Classification V2/Valid\",\n",
        "    label_mode='int',\n",
        "    image_size=(IMAGE_HEIGHT, IMAGE_WIDTH),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=None #TODO: Set appropriate value\n",
        ")\n",
        "\n",
        "# Load the test dataset\n",
        "test_data = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    \"Fast Food Classification V2/Test\",\n",
        "    label_mode='int',\n",
        "    image_size=(IMAGE_HEIGHT, IMAGE_WIDTH),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=None #TODO: Set appropriate value\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mpoA40RHkPg8"
      },
      "source": [
        "2. Add normalization and data augmentation as needed to the datasets.  \n",
        "Which normalization is best? (Tip: Check how the original model was trained.)  \n",
        "Which data augmentation makes sense?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "_Kd1f6CKkPg8"
      },
      "outputs": [],
      "source": [
        "# Training-specific preprocessing\n",
        "train_preprocessing = tf.keras.Sequential([\n",
        "    tf.keras.layers.Rescaling(1./255),  # Normalize pixel values to [0, 1]\n",
        "    tf.keras.layers.RandomFlip(\"horizontal_and_vertical\"),  # Data augmentation\n",
        "    tf.keras.layers.RandomRotation(0.2),  # Data augmentation\n",
        "    tf.keras.layers.RandomZoom(0.2),  # Data augmentation\n",
        "])\n",
        "\n",
        "# Validation-specific preprocessing\n",
        "validation_preprocessing = tf.keras.Sequential([\n",
        "    tf.keras.layers.Rescaling(1./255),  # Normalize pixel values to [0, 1]\n",
        "])\n",
        "\n",
        "# Helper functions to apply preprocessing only to images, not labels\n",
        "def preprocess_train(image, label):\n",
        "    return train_preprocessing(image), label\n",
        "\n",
        "def preprocess_validation(image, label):\n",
        "    return validation_preprocessing(image), label\n",
        "\n",
        "# Apply preprocessing to the datasets\n",
        "train_data_small = train_data_small.map(preprocess_train)\n",
        "validation_data = validation_data.map(preprocess_validation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-dNVt06Gz828"
      },
      "source": [
        "## 3. Model Definitions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HkEKsc7CW4-0"
      },
      "source": [
        "Imports and Constants\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "fpJXUjNZW4-0"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "CLASSES = [\"Taquito\", \"Crispy Chicken\", \"Donut\"]\n",
        "IMAGE_HEIGHT = 128\n",
        "IMAGE_WIDTH = 128\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UEFeJj34W4-0"
      },
      "source": [
        "### Feature Extraction\n",
        "\n",
        "1. Create a feature extraction model using the Inception V3 architecture.\n",
        "2. Freeze all layers of the base model.\n",
        "3. Add a new classification head with global average pooling, dropout, and a dense layer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "BYsDXojA0Wm3"
      },
      "outputs": [],
      "source": [
        "def create_feature_extraction_model():\n",
        "    \"\"\"Creates model with frozen base layers (feature extraction)\"\"\"\n",
        "    \n",
        "    # Load pre-trained InceptionV3 with correct input size\n",
        "    base_model = tf.keras.applications.InceptionV3(\n",
        "        weights='imagenet',\n",
        "        include_top=False,\n",
        "        input_shape=(IMAGE_HEIGHT, IMAGE_WIDTH, 3)\n",
        "    )\n",
        "\n",
        "    # Freeze all layers for feature extraction\n",
        "    base_model.trainable = False\n",
        "\n",
        "    # Simple classification head\n",
        "    # - GlobalAveragePooling2D reduces spatial dimensions\n",
        "    # - Final Dense layer maps to class probabilities\n",
        "    model = tf.keras.Sequential([\n",
        "        base_model,\n",
        "        tf.keras.layers.GlobalAveragePooling2D(),\n",
        "        tf.keras.layers.Dropout(0.2),\n",
        "        tf.keras.layers.Dense(CLASSES, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KUK36M-NW4-1"
      },
      "source": [
        "### Fine-Tuning Last Few Layers\n",
        "\n",
        "1. Create a fine-tuning model using the Inception V3 architecture.\n",
        "2. Unfreeze the last few layers of the base model.\n",
        "3. Add a new classification head with global average pooling, dropout, and a dense layer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "_khIIV-zW4-1"
      },
      "outputs": [],
      "source": [
        "def create_fine_tuning_model():\n",
        "    \"\"\"Creates model with last few layers unfrozen for fine-tuning\"\"\"\n",
        "\n",
        "    # Load pre-trained InceptionV3 with correct input size\n",
        "    base_model = tf.keras.applications.InceptionV3(\n",
        "        weights='imagenet',\n",
        "        include_top=False,\n",
        "        input_shape=(IMAGE_HEIGHT, IMAGE_WIDTH, 3)\n",
        "    )\n",
        "\n",
        "    # Freeze all layers except last few blocks\n",
        "    base_model.trainable = True\n",
        "    for layer in base_model.layers[:-20]:  # Unfreeze last 20 layers\n",
        "        layer.trainable = False\n",
        "\n",
        "    model = tf.keras.Sequential([\n",
        "        base_model,\n",
        "        tf.keras.layers.GlobalAveragePooling2D(),\n",
        "        tf.keras.layers.Dropout(0.2),\n",
        "        tf.keras.layers.Dense(CLASSES, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPRIXMlxW4-2"
      },
      "source": [
        "### Full Fine-Tuning\n",
        "\n",
        "1. Create a fine-tuning model using the Inception V3 architecture.\n",
        "2. Unfreeze all layers of the base model.\n",
        "3. Add a new classification head with global average pooling, dropout, and a dense layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "troP8F0VW4-2"
      },
      "outputs": [],
      "source": [
        "def create_full_fine_tuning_model():\n",
        "    \"\"\"Creates model with all layers unfrozen for full fine-tuning\"\"\"\n",
        "\n",
        "    base_model = tf.keras.applications.InceptionV3(\n",
        "        weights='imagenet',\n",
        "        include_top=False,\n",
        "        input_shape=(IMAGE_HEIGHT, IMAGE_WIDTH, 3)\n",
        "    )\n",
        "\n",
        "    # Make all layers trainable\n",
        "    base_model.trainable = True\n",
        "\n",
        "    model = tf.keras.Sequential([\n",
        "        base_model,\n",
        "        tf.keras.layers.GlobalAveragePooling2D(),\n",
        "        tf.keras.layers.Dense(1024, activation='relu'),\n",
        "        tf.keras.layers.Dropout(0.2),\n",
        "        tf.keras.layers.Dense(CLASSES, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVds6rKVW4-2"
      },
      "source": [
        "## 4. Model Training\n",
        "\n",
        "## Tasks\n",
        "\n",
        "1. Optimize two different models either using the same transfer learning strategy and two of the three datasets with different sizes or alternatively use one of the datasets with two different learning strategies.\n",
        "\n",
        "2. Find out what the arguments of the early stopping callback function mean.\n",
        "\n",
        "3. Choose appropriate learning rates.\n",
        "Tips for choosing it:\n",
        "- Start with the Default: Begin with the Adam optimizer's default learning rate of 0.001. This is often a good starting point.\n",
        "- When fine-tuning (especially full fine-tuning), consider using a smaller learning rate than the default. This helps prevent large updates to the pre-trained weights, which could disrupt the learned features. A learning rate of 1e-4 or 1e-5 is a good starting point.\n",
        "- The best learning rate will depend on the specific dataset and model. Experiment with different values and observe the training and validation performance. Look for a learning rate that allows the model to converge smoothly without overshooting or getting stuck in a local minimum.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xEvwEFNfSRtR"
      },
      "source": [
        "### Example for Training of the Feature Extraction Model with the Small Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dN9XU7xYdW2n"
      },
      "outputs": [],
      "source": [
        "LEARNING_RATE = 0.001\n",
        "\n",
        "# Settings for the Feature Extraction Model on the small dataset\n",
        "model_feature_extraction_small = create_feature_extraction_model()\n",
        "# Compile the model\n",
        "model_feature_extraction_small.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "# Model Summary\n",
        "model_feature_extraction_small.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "YoHtBg-0W4-2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 7s/step - accuracy: 0.3108 - loss: 2.9629 - val_accuracy: 0.4036 - val_loss: 1.6723\n",
            "Epoch 2/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5s/step - accuracy: 0.4630 - loss: 2.0343 - val_accuracy: 0.3473 - val_loss: 2.4803\n",
            "Epoch 3/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5s/step - accuracy: 0.2434 - loss: 3.3988 - val_accuracy: 0.3536 - val_loss: 2.2839\n",
            "Epoch 4/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 5s/step - accuracy: 0.2796 - loss: 2.8630 - val_accuracy: 0.4382 - val_loss: 1.4680\n",
            "Epoch 5/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 5s/step - accuracy: 0.3179 - loss: 2.2042 - val_accuracy: 0.5418 - val_loss: 1.1730\n",
            "Epoch 6/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 5s/step - accuracy: 0.4362 - loss: 1.6839 - val_accuracy: 0.5309 - val_loss: 1.2317\n",
            "Epoch 7/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 5s/step - accuracy: 0.4636 - loss: 1.5774 - val_accuracy: 0.5682 - val_loss: 1.0899\n",
            "Epoch 8/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 5s/step - accuracy: 0.4867 - loss: 1.6130 - val_accuracy: 0.6082 - val_loss: 0.9892\n",
            "Epoch 9/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5s/step - accuracy: 0.4144 - loss: 2.0015 - val_accuracy: 0.5991 - val_loss: 1.0374\n",
            "Epoch 10/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 5s/step - accuracy: 0.4999 - loss: 1.6240 - val_accuracy: 0.6200 - val_loss: 0.9811\n"
          ]
        }
      ],
      "source": [
        "# Define and compile the model if not already done\n",
        "model_feature_extraction_small = create_feature_extraction_model()\n",
        "model_feature_extraction_small.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Model Training\n",
        "history_feature_extraction_small = model_feature_extraction_small.fit(\n",
        "    train_data_small,\n",
        "    validation_data=validation_data,\n",
        "    epochs=10,\n",
        "    callbacks=[\n",
        "        tf.keras.callbacks.EarlyStopping(\n",
        "            monitor='val_accuracy',\n",
        "            patience=3,\n",
        "            restore_best_weights=True\n",
        "        )\n",
        "    ]\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
